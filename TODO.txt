
Todo:
	2) cache the current board state in a reference in the Node class so we don't have to replay the entire game all the time. that should speed things up.
	3) bug: it seems like the two players are ending up playing different games, too, sometimes toward the end...
		slight refinement to that bug... the tree search is sometimes recommending illegal moves!
		i'm counting simulation errors now and it'll be fine up to a point and then all hell breaks loose!
		Possible Remedies/Diagnostics:
			cache intermediate board states and verify that they match with the globally unrolled version
			return an arraylist of sensible moves instead of an array of moves that may or may not be sensible (also, it may speed things up toward end-game play just a tad)


	3) add functionality to extract gamestate at each node so that i can actually build a dataset
	4) build the functionality needed to play tournaments with the specifications outlined below
		this includes the functionality to write a training-ready dataset to file
	5) develop a model in python that can train on the exported dataset
	
	that should really be enough for today!
	
	6) try and figure out how to load keras models in java as this is probably the easiest way to initiate the training feedback loop
	7) train the current and next architectures in parallel and switch over when the larger one is able to beat the smaller one like 2/3 of the time or something similarly overwhelming

Issue:
	There may be a problem with how the "game from lineage" function is working. it looks like sometimes i don't get the same game for black and white
	Maybe make a function that records the previous state for comparison and shows me the diff and I can sit there and manually verify..? 
	This may be a case where I should sit down and just design tests for all this stuff...

Task:
	Add "pass" to the list of possible moves in both the tree search and the model (can make it equal to the area of the board or something)
		Note: "pass" is the only move that's always legal ;)

Task:
	be able to export training data
		write functions for actually automating gameplay and exporting the results to a file

Task:
	consider adding a reference to the Node class to hold an iGo
		this will enable caching of intermediate states of the game and should significantly speed up the tree search in later stages
		it should also give me a convenient avenue to test both the copy constructor for iGo and the lineageToMoveset function in the mcts class

Task:
	One of my favorite features of KataGo -- variable board sizes
	Definitely leverage this in self-play games. I've already made a spreadsheet outlining a rough training schedule.
	
	Training Schedule

		Step 1: 
			At this stage, we have a randomly initialized model, so there's really no point in using a model at all. The authors of the KataGo paper showed in their ablation study that games played with only 100 steps of lookahead were still good for training. This seems to be because the value head of the network is data-hungry and that the value head drives policy selection to a large degree early in the training process. The way we develop an understanding of value is to ground it in the final score of the game and this slowly "percolates" up the tree search as the value head sees more and more training data. 
			
			So I'll do the following:
				1) select a board size from {7,8,9} using a weighted random sample
					these boards are small enough to be fast to simulate, but large enough to be dynamic

				2) play with a random number of rollouts sampled from 2^{normal(mu=6, sigma=2)}
					i feel like having a random number of rollouts basically makes the policy smarter or dumber
					this will hopefully lead to more variability in gameplay and the model will be able to learn to resolve a larger variety of states
				
				3) i think i'll generate 100,000 state-action pairs, then i'll train a model (32x4)
					i'm currently generating about 8 moves/second across that set of {7,8,9}, so i'm happy to wait for 100,000 of those
				
				4) once i'm training with a model, i'll retrain after every new batch of 100,000 moves.
					i'll have a window of maybe a million moves (maybe 10M?)
					models are expensive, so i'll pad my data by generating symmetries. this way, i'll only have to generate 12,500 samples per training/retraining cycle
					i'll save one model checkpoint per training cycle
					i'll have the latest checkpoint face off against the strongest model in a simple-majority wins competition for strongest model

			Training Data Format?
				int(edge) + '\t' + ';'.join([moveToSGF(mv) + '|' + int(policy) + '|' + %f5(value) for mv, policy, value in moveset])
				this way, we're kind of explicit about the fact that the policy and value correspond to this state -- the value of this state is x and the thing we do next is y
				i just need to write a function that kind of does the same thing that lineageToMoveset does except maybe call it lineageToDataset and include the value at each node as well

				or
				
				i could output the following and it'd be much simpler...
					movesInStack + '\t' + boardState + '\t' + int(policy) + '\t' + %f5(value) '\n'
					each game would start with all zeros, the policy == whatever the next move was, and value == 0.5 because we just assume the komi was set fairly
					don't even worry about compressing the board state for now, even though you'll probably need to eventually...
				
				there are also a few training objectives to consider...
				1) vanilla -- policy, value
				2) territory -- policy, value, tromp-taylor outcome
				3) fancy -- policy, value for this step and the next 5 steps probably by applying a really short-sequence lstm or something

			a little disorganized...
			
			Anyway, i need to:
				1) add "pass" as a valid option that's always legal (in mcts and in the iGo class where i'll just add "area" to the -1 condition to see if it's a pass)
				2) add functionality that, once the game is concluded, conditional on whether it concludes because it exceeded movecount or if it concluded "honestly", we call lineageToDataset and then we write the dataset for the game to file and start another game -- i can develop a model in python just to test how well it does with these data predicting policy and value

Task:
	General Training Outline:

		Initialize Training:
			Assumptions:
				I have a root directory to work from, but
				I have no training data and no trained models
				I only have the rules of the game and a generic policy augmentation algorithm (mcts)
			
			Actions:
				I create some empty log files like this:
					1) log file describing the current state of the training data (where a particular file is, when it was created, and what method/model was used to create it)
					2) log file describing the current state of my "model zoo" (when models were created and what data they've been trained on (saved once per checkpoint))
					3) log file describing tournaments between models including win/loss rates and when a new models is selected to be the "strongest model"
				Copy over a training schedule, for example:
					One training cycle consists of:
						1) selecting a policy model - if no trained model is available, default to vanilla mcts
						2) 	if using vanilla mcts, then play through 12,500 games per checkpoint,
							else if a model is available, play through 1250 games per checkpoint
						3) 	if using vanilla mcts, then sample games from the distribution {7,8,9} in inverse proportion to board area,
							else if a model is available, then samples games from the distribution {9,10,11,12,13,14,15,16,17,18,19} in inverse proportion to board size

						encouraging variability in gameplay
							4) komi is set on a per game basis according to 6.5+int(normal(0,1)), but we play with 6.5 -- this way, we hopefully don't train the model to just eek out a narrow victory
							5) the number of playouts is sampled from the distribution 2^{normal(mu=7, sigma=1)}
							6) take a weighted random sample from the top 3 moves for each of the first {boardSize} moves

						training inputs and objectives
							7) train a model by uniformly sampling the last 100,000 games (before accounting for symmetries)
							8) training inputs (5 channels currentState(black, white)+previousState(black,white)+playerToMove)
							9) training objectives (one-hot policy, regression utility, ternary tromp-taylor score (black, white, neutral))						
						
						
				I initiate the training loop
		
		Training Loop:
			1) load a policy (vanilla mcts or model checkpoint)
			2) play the required number of games under the required conditions, save training samples, update logs
			3) train the current model (on k training samples) by uniformly sampling the last n games -- always sampling symmetries
			4) save the model checkpoint and do a tournament against the current strongest policy to see if the new checkpoint should replace it
			5) update logs
			6) go to step 1




