
Todo:
	4) go ahead and write something in python that can train on these data and see how we go
		pretty sure i'm at the point now where i can begin fitting a model...
	
	5) try and figure out how to install dl4j and load/do inference with a keras model trained in python



	1) add a resignation threshold of 5% like they did with alphago. that should save some time.
		make it so you can just add a parameter that'll increase the likelihood of passing if the utility function sucks
	
	2) put together a class that can run games where each player uses different strategies
		such a class can be used to generate training data and can be used to figure out relative strength between one policy and another

	5) run multiple instances of this on the big computer to generate lots of training data to get a model started

	
	that should really be enough for today!
	
	6) try and figure out how to load keras models in java as this is probably the easiest way to initiate the training feedback loop
	7) train the current and next architectures in parallel and switch over when the larger one is able to beat the smaller one like 2/3 of the time or something similarly overwhelming

Task:
	Training Schedule

			So I'll do the following:
				2) play with a random number of rollouts sampled from 2^{normal(mu=6, sigma=2)}
					i feel like having a random number of rollouts basically makes the policy smarter or dumber
					this will hopefully lead to more variability in gameplay and the model will be able to learn to resolve a larger variety of states
				
				4) once i'm training with a model, i'll retrain after every new batch of 100,000 moves.
					i'll have a window of maybe a million moves (maybe 10M?)
					models are expensive, so i'll pad my data by generating symmetries. this way, i'll only have to generate 12,500 samples per training/retraining cycle
					i'll save one model checkpoint per training cycle
					i'll have the latest checkpoint face off against the strongest model in a simple-majority wins competition for strongest model

Task:
	General Training Outline:

		Initialize Training:
			Assumptions:
				I have a root directory to work from, but
				I have no training data and no trained models
				I only have the rules of the game and a generic policy augmentation algorithm (mcts)
			
			Actions:
				I create some empty log files like this:
					1) log file describing the current state of the training data (where a particular file is, when it was created, and what method/model was used to create it)
					2) log file describing the current state of my "model zoo" (when models were created and what data they've been trained on (saved once per checkpoint))
					3) log file describing tournaments between models including win/loss rates and when a new models is selected to be the "strongest model"
				Copy over a training schedule, for example:
					One training cycle consists of:
						1) selecting a policy model - if no trained model is available, default to vanilla mcts
						2) 	if using vanilla mcts, then play through 12,500 games per checkpoint,
							else if a model is available, play through 1250 games per checkpoint
						3) 	if using vanilla mcts, then sample games from the distribution {7,8,9} in inverse proportion to board area,
							else if a model is available, then samples games from the distribution {9,10,11,12,13,14,15,16,17,18,19} in inverse proportion to board size

						encouraging variability in gameplay
							4) komi is set on a per game basis according to 6.5+int(normal(0,1)), but we play with 6.5 -- this way, we hopefully don't train the model to just eek out a narrow victory
							5) the number of playouts is sampled from the distribution 2^{normal(mu=7, sigma=1)}
							6) take a weighted random sample from the top 3 moves for each of the first {boardSize} moves

						training inputs and objectives
							7) train a model by uniformly sampling the last 100,000 games (before accounting for symmetries)
							8) training inputs (5 channels currentState(black, white)+previousState(black,white)+playerToMove)
							9) training objectives (one-hot policy, regression utility, ternary tromp-taylor score (black, white, neutral))						
						
						
				I initiate the training loop
		
		Training Loop:
			1) load a policy (vanilla mcts or model checkpoint)
			2) play the required number of games under the required conditions, save training samples, update logs
			3) train the current model (on k training samples) by uniformly sampling the last n games -- always sampling symmetries
			4) save the model checkpoint and do a tournament against the current strongest policy to see if the new checkpoint should replace it
			5) update logs
			6) go to step 1




